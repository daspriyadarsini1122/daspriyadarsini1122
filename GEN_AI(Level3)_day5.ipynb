{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM08HxRn7UU8CgtC+JsHMUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daspriyadarsini1122/daspriyadarsini1122/blob/master/GEN_AI(Level3)_day5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**classification**\n",
        "\n",
        "- bank loan payment prediction\n",
        "- Credit risk assessment KNN\n",
        "- email spam detection\n",
        "- Sentiment detection\n",
        "\n",
        "2. university admission prediction\n",
        "3. Diabetes onset prediction(Pima indians diabetes dataset)\n",
        "4. iris flowers classification\n",
        "\n",
        "- regression, correlation, and causation relates to data science but ML is predicting the future."
      ],
      "metadata": {
        "id": "LvTH1g-Laj7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ----------------- Load Dataset -----------------\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "columns = [\n",
        "\n",
        "    \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
        "\n",
        "    \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"\n",
        "\n",
        "]\n",
        "\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "print(\"Data shape:\", df.shape)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# ----------------- Correlation Analysis -----------------\n",
        "\n",
        "corr = df.corr()\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Correlation of input features with Outcome\n",
        "\n",
        "print(\"\\nCorrelation with Outcome:\")\n",
        "\n",
        "print(corr[\"Outcome\"].sort_values(ascending=False))\n",
        "\n",
        "# ----------------- Regression (Logistic) -----------------\n",
        "\n",
        "X = df.drop(\"Outcome\", axis=1)\n",
        "\n",
        "y = df[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic regression model\n",
        "\n",
        "logreg = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ----------------- Statistical Significance & Causation -----------------\n",
        "\n",
        "X2 = sm.add_constant(X)\n",
        "\n",
        "model = sm.Logit(y, X2)\n",
        "\n",
        "result = model.fit(disp=0)\n",
        "\n",
        "print(\"\\nStatistical Summary (Logistic Regression):\")\n",
        "\n",
        "print(result.summary())\n",
        "\n",
        "# Interpretation: Features with p-values < 0.05 are statistically significant predictors of diabetes.\n",
        "\n",
        "# Note: Correlation does not imply causation, but significant features (like Glucose, BMI, Age)\n",
        "\n",
        "# are strong indicators of causal influence according to medical research.\n",
        ""
      ],
      "metadata": {
        "id": "5IKLrSP25s2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Summary\n",
        "from pandas import read_csv\n",
        "import matplotlib\n",
        "import numpy\n",
        "fkrom matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "rfrom pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "srKB63aGc7RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"https://github.com/enuguru/DataScienceLevelOne/blob/master/pydslevelone/data_analysis/pima_indians_dataset.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "opima = read_csv('pima_indians_dataset.csv', names = names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "GSOGZxdkn-iM",
        "outputId": "d1f26087-75e7-47a8-e7f5-3259a80c1bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'pima_indians_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1096022384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/enuguru/DataScienceLevelOne/blob/master/pydslevelone/data_analysis/pima_indians_dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'preg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pres'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'skin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pedi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pima_indians_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pima_indians_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "- https://gist.githubusercontent.com/ktisha/c21e73a1bd1700294ef790c56c8aec1f/raw/819b69b5736821ccee93d05b51de0510bea00294/pima-indians-diabetes.csv\n",
        "- https://github.com/enuguru/aiandml/blob/master/text_files/datasets/csv_files_links_and_locations.txt"
      ],
      "metadata": {
        "id": "N_82OI8f5Fjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A prompt to ChatGPT**\n",
        "\n",
        "- Take the pima indians diabetes dataset from the internet and build regression, correlation between input variables and output variable and find causation if any. Give the python program for doing this analysis as well."
      ],
      "metadata": {
        "id": "PHBG1ok5v6WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural networks**\n",
        "\n",
        "Why study neural networks & deep learning? We already have so many techniques in conventional machine learning?\n",
        "\n",
        "- Whenever you have a problem that is difficult to solve look towards nature. You can not solve a problem at the same level at which you created it(Albert Einstein)"
      ],
      "metadata": {
        "id": "OPAZuZ6m2icx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neurons**\n",
        "\n",
        "- Neuronsw are just nerve cells that send nerve signals to and from the brain and with each other at up to maximum speed of 200 mph.\n",
        "- The neturon has a cell body( or soma) with branching dendrites(called signal receivers) and a projection called an axon, which conduct the nerve signal.\n",
        "- We have 50 to 70 trillion cells in human body and each neuron is a special type of cell, that is a nerve cell.\n",
        "- In that there are 85 billion neurons inside our brain.\n",
        "- The seignals emanating from the auditory cortex, visual cortex and sensory cortex reach these neurons through the nervesn that conduct these signals.\n",
        "- Our brain is made of around 100 billion nerve cells, and they are called neurons. Neurons have the wonderful ability to co llect and tre ansmirt electrochemical signals -- think of them like our gates tand wires of a PC.\n",
        "- Neurons share the same properties having the same makeup as other cells.The electrochemical aspect allows them to sendl signals over big distances(maximum of several feet or a couple of meters) and also send messages among each other.\n",
        "eura\n",
        "- More than two neural network is deep learning neural network.\n"
      ],
      "metadata": {
        "id": "elV3CRCV_-V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We use 5 methods for these neural network**\n",
        "- Sigmoid, tanh, relu, linear, and softmax\n",
        "- Softmax function is great for classification problems,especially if you are dealing with multi-class classification problems, as it will report back the \" confidencei score\" for each class. Since we are dealing with probabilities here, the scores returned by the softmax function will add upto 1.\n",
        "- We use sigmoid activation function for binary classification and multi level classification.\n",
        "- We use softmax activation function for multi class classification.\n",
        "- We use linear activation function for regression problems.\n",
        "- Inside a neuron, we have three things ( Sigma, activation function, output function)\n",
        "- neural_network_simple/fpnn/forward_propagate_2_1_1.ipynb"
      ],
      "metadata": {
        "id": "8gd7U4FGIhwM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3CvM6rKptN2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}