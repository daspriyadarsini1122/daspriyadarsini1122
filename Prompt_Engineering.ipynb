{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu3VXroLalZRDAroENgVCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daspriyadarsini1122/daspriyadarsini1122/blob/master/Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Prompt Engineering**\n",
        "\n",
        "- Different types of Conversational Agent - (ChatGPT, Google Bard, Bing chat, Perplexity.AI etc)\n",
        "- Different types of prompting -\n",
        "1. Zero-shot prompting:- Its like a general query , for ex: Give me some movies to watch, then if we want to refine the output we go for\n",
        "2. Few-shot prompting:- Here we give some example in prompt like i like this kind of movies , suggest me some new movies to watch, we can also improve it by some more refinement , then we will go for\n",
        "3. Chain of though prompting:- It solves the complex math problem or complex code generation step by step\n",
        "4. Another type of prompting can be used i.e Augmented knowledge prompting:- Here we give the model number of relevant facts that the model needs to use to tailor its response, Here the additional facts what we give we augment the model & the model can generate the response.\n",
        "\n",
        "**Some Rules**\n",
        "\n",
        "- Put instructions at the beginning and separate instructions from text using ###\n",
        "- Provide examples for what you want the output look like\n",
        "- Start with zero-short , then use few-shot and then fine-tune the model\n",
        "- Specify what to do rather than what not to do, try positive constraint\n",
        "- For code generation use leading words to guide the model in the right direction ( like import for code or select for sql)"
      ],
      "metadata": {
        "id": "sFnEOExPZq6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Generative AI & LLMs**\n",
        "\n",
        "- Generative AI uses a powerful machine learning(ML) model  to learn patterns and relationships in a dataset created by humans.\n",
        "- The model (Which is just a mathematical representation) uses learned data to create new content.\n",
        "- New content resembles the content that the model has already seen before.\n",
        "- ChatGPT & DALL-E are referred to as models, but they are the interfaces to the underlying models that power them.\n",
        "- ChatGPT is a chatbot built using a text-to-text model called GPT(generative pre-training transformer)\n",
        "- DALL-E is the interface to a text-to-image model that can generate images from text prompts\n",
        "- The underlying model is a mathematical construct that has been trained on massive amounts of data.\n",
        "- Each word produced in the output is used by the large langauge model to predict the next word in the sequence.This is a massive auto completion mechanism.\n",
        "- They are large because they have billions of parameters that need to be trained & they are trained on very large corpus of data comprising billions of records.\n",
        "- All of these LLMs are build using neural networks. Neural network also means deep learning model or simply a type of machine learning model.\n",
        "- There are different types of neural networks:\n",
        "1. Convolutional neural networks are used to work with images.\n",
        "2. Recurrent neural networks are used to work with time series or sequential data.\n",
        "- The breakthrough in LLm came with the. reation of transformer architecture, a certain kind of architecture for a neural network.\n",
        "- Transformer networks are sequence-to-sequence models.\n",
        "- They use the concept of attention to focus on the right parts of the input sequence.(Like in a senetence there must be some portion , which is important)\n",
        "- The word is a combination of Chatbot(interface) + GPT(brains)\n",
        "- The chatbot is built using a very powerful LLM that relies on the GPT architecture(i.e it knows how to pay attention to the important parts of the input text sequence)\n",
        "- ChatGPT is just the app that uses any version of the GPT language model behind the scenes"
      ],
      "metadata": {
        "id": "3n9vPeBHoy1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducing the OpenAI Playground**\n",
        "\n",
        "- ChatGPT is a web based application built on the top of GPT-3.5 and GPT-4. Its a type of Chatbot that was released by OpenAI.\n",
        "- OpenAI also makes available developer APIs that you can use to integrate your application with ChatGPT.\n",
        "- Can use these APIs to build applications and integrate natural language conversation into any product that you offer to your consumer.These APIs is available to all who do a sign of for the payment of a fee.\n",
        "- If not want to write the code , just explore & research , then no use of API directly, Instead use OpenAI Playground.\n",
        "- The OpenAI Playground is a web interface that allows users to interact with OpenAI's GPT models\n",
        "- platform.openai.com/playground\n",
        "- These pricing is decided based on the tokens.\n",
        "- The tokens are pieces of words used for natural language processing. For English text, 1 token is approx. 4 characters or 0.75 words."
      ],
      "metadata": {
        "id": "6CVZsUq2PEKb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdQBF-j-_kEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}